### IEEE-754标准与浮点数运算



#### 计算机"眼中"的数字

计算机内数据和指令都是由晶体管和门电路等元件完成的，对于这些元件来说，`开`或者`关`是其唯一的状态，这种状态的表现就是二进制的理念。就像黑客帝国世界中漫天飞的`0`和`1`一样,计算机世界使用的**机器语言**也只有`0`和`1`。而在**机器语言**中，当计算机想要表示一个数字时，这时就得使用**机器数**了。

> 机器数（computer number）是将符号"数字化"的数，是数字在计算机中的二进制表示形式。机器数有2个特点：一是符号数字化，二是其数的大小受机器字长的限制

机器数有两个特点：

1. 符号数字化，通常机器数以最高位表示正负号，即`0`代表正数，`1`代表负数。
2. 数值的大小受字长显示，如8bit的机器，其可表示的**真值**数值范围为`-127`~`+127`，**机器数**即`11111111`~`01111111`。这里其实有关于原码、反码、补码及其计算的概念，如果有兴趣可以跳转这里。

那么我们以8bit二进制为例,看看**机器数**如何表示10进制中的`3`和`-3`。

```
对于正数符号位为0,所以机器数为00000011
对于负数符号位为1,所以机器数为10000011
```



#### IEEE标准的由来

在上文中，我们知道计算机通过**二进制**可以精确的表达整数，那么计算机是如何表达小数，或者说计算机是如何处理这些小数的呢？

在早期，计算机表示小数的方式是**定点小数**的方式去表示的，**定点小数**中。小数点隐含在第一位编码和第二位中间。

```
如真值为正值的小数0.101,使用定点小数在计算机中表示就为0 101
```

> 然而，早期使用定点小数无法表示过大和过小的值，并且在计算过程由于数值范围的限定，会出现数值溢出的问题。

随着技术的更新，在1978年的时候，Intel公司推出了首枚**16bit微处理器(CPU)[8086](https://en.wikipedia.org/wiki/Intel_8086)**。这台**x86**的老祖宗虽然自身无法处理小数的运算，但是在编译器层面可以通过用整数指令模拟出小数的运算，但是这种运算的方式效率是非常低的。

为了解决这类问题，1980年Intel公司推出了首款**x87浮点协处理器运算单元(FPU)[8087](https://en.wikipedia.org/wiki/Intel_8087)**，通过主板上额外的**协处理器插槽**，安装后不仅可以解决小数的运算问题，并且对于不同的应用，性能提升了20%~500%。

对于计算机发展来说，**8087**是款非常棒的**FPU**，但是它的意义真正体现在这款**FPU**的设计师之一的**William Kahan**教授设计了**IEEE-754标准**的雏形，而正是因为这套标准，我们计算机才能精准的处理小数。

1985年时，IEEE推出了[IEEE 754-1985](https://en.wikipedia.org/wiki/IEEE_754-1985)标准，随着大佬们的努力，IEEE还推出了目前的版本——[IEEE 754-2008](https://en.wikipedia.org/wiki/IEEE_754-2008_revision)。

而我们使用的高级语言中浮点数的运算，如JavaScript、Java都是基于这个标准而定。

